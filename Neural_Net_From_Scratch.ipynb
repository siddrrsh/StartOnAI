{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Net_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddrrsh/StartOnAI/blob/master/Neural_Net_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwYJsb6MjDbW"
      },
      "source": [
        "# **Neural Network from Scratch Tutorial in Python**\n",
        "###### Created by **(Karthik Bhargav, Keshav Shah, Sauman Das)** for [StartOnAI](https://startonai.com/)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbqPIhRQNWWF"
      },
      "source": [
        "#Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srB7hSW32oJu"
      },
      "source": [
        "We will cover the following topics in this notebook.\n",
        "\n",
        "\n",
        "*   Theory of how Perceptrons work and Learn\n",
        "*   Coded Walkthrough of a Neural Network\n",
        "*   Model the Wisconsin Breast Cancer Dataset\n",
        "*   Review Various Applications of Neural Networks\n",
        "\n",
        "So stay tuned!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOr3gaeovftN"
      },
      "source": [
        "# What Are Neural Networks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESAIluzExhen"
      },
      "source": [
        "In simple terms, neural networks are representative of the human brain, and they are specificially made to recognize patterns. They interpret data through various models. The patterns that these models detect are all numerical specifically in the form of vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SamhlETV1ZTd"
      },
      "source": [
        "Neural networks are extremely helpful for performing tasks involving clustering and classification. Because of the networks similarity to the human brain, it is able to recognize patterns in unlabeled data.\n",
        "\n",
        "We will start off by investigating the most basic Neural Network: **The Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElDyoWNlFOv"
      },
      "source": [
        "## Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3dDubF6LUsh"
      },
      "source": [
        "<img src=\"https://tinyurl.com/ybcfd78e\" alt=\"perceptron\" width=\"400\"/>\n",
        "\n",
        "[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvT_Nv-AXCN"
      },
      "source": [
        "The Perceptron consists of two main components\n",
        "1.   Neurons ($x_i$)\n",
        "2.   Weights ($w_i$)\n",
        "\n",
        "Perceptrons represent the most basic form of a Neural Network with only two layers, the input and output layer.  As shown in the diagram above, both layers are joined by weights represented by the arrows. Each individual neuron represents a number. For example, if there are three inputs, the input layer will consist of 3 neurons plus an additional bias neuron. The importance of the bias ($b$) will become clear later in this tutorial. The output layer simply consists of one neuron in this scenario which represents the number we are attempting to predict. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyhmPbDzUspC"
      },
      "source": [
        "##Forward Propagation\n",
        "\n",
        "The process of going from the input layer to the output is known as Forward Propagation. To simplify the computations, we will use vector notation to represent the input features and the weights.\n",
        "\n",
        "  $\\vec{x}=\\begin{bmatrix}  x_1 & x_2 & ... & x_n\\end{bmatrix}$\n",
        "\n",
        "\n",
        "  $\\vec{w}=\\begin{bmatrix}  w_1 & w_2 & ... & w_n \\end{bmatrix}$\n",
        "\n",
        "  Finally, to get the value of the output neuron, we simply take the dot product of these two vectors and add the bias. \n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibfm7azdc5-2"
      },
      "source": [
        "###The Bias Term\n",
        "\n",
        "To get a better understanding of this output, lets analyze it with just one input neuron. In other words, our output neuron will store the following.\n",
        "\n",
        "$z=x_1\\times w_1+b$\n",
        "\n",
        "If we visualize this in two dimensional space, we know that this will represent a line with slope $w_1$ and intercept $b$. We can now easily see the role of the bias. Without it, our model would always go through the origin. Now, we can shift our model along the axes giving us more flexibility while training. However, we are still only able to represent linear models. To add non-linearities to our model we use an activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdJxEI9hf9T"
      },
      "source": [
        "###Activation Functions\n",
        "\n",
        "Lets imagine that we are solving a binary classification problem. This means the range of our output $\\hat{y}$ (predicted value) must be $(0, 1)$ since we are predicting a probablity that the input belongs to a certain class. However, the range of a linear equation is $(-\\infty, \\infty)$. Therefore, we must apply some other function to satisfy this constraint. In binary classification problems, the most common activation function is called the sigmoid function. \n",
        "\n",
        "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "\n",
        "<img src=\"https://tinyurl.com/ycggxehs\" alt=\"sigmoid_graph\" width=\"400\"/>\n",
        "\n",
        "As you can see in this graph, $\\sigma(x)\\in(0, 1)$. This activation function makes it possible to predict a probablity for a binary output. As you go further into machine learning, you will see several other activation functions. The most common ones other than sigmoid are ReLU, tanh, and softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2F37Lp-jHU"
      },
      "source": [
        "###The Output\n",
        "\n",
        "Now that we know all the parts of the perceptron, let's see how to get the final output. After forward propagation, we saw the output was\n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "Finally, we must apply the activation function to get our final output.\n",
        "\n",
        "$\\hat{y}=\\sigma(z)$\n",
        "\n",
        "That is all there is to get the output from a perceptron! To sum it up in three simple steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Get the dot product of the weights and the input features $(\\vec{x}\\cdot\\vec{w})$.\n",
        "2.   Add the bias $(\\vec{x}\\cdot\\vec{w}+b)$.\n",
        "3.   Apply the activation function and that is the predicted value $(\\hat{y}=\\sigma(\\vec{x}\\cdot\\vec{w}+b))$!\n",
        "\n",
        "So far we know how to take the input values and return the corresponding output. However, we must adjust the weights to make the network fit the training data. The process of making these adjustments is known as **back propagation**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_AwZqZQmBzq"
      },
      "source": [
        "In order to adjust our weights, first we must figure out a way to numerically signify the accuracy of our prediction. In other words, we need to figure out how close our predicted value is to the actual value. A simple way to do this is to use the **Sum of Squares Error**.\n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=(y-\\hat{y})^{2}$\n",
        "\n",
        "Although this function works, most real-life applications will not use this error function. We will discuss another group of cross entropy loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g-R2gHbEw5R"
      },
      "source": [
        "##Loss Function\n",
        "\n",
        "Several functions exist for accomplishing this task, however, the most common loss function for binary problems is called **Binary Cross-Entropy**.\n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=-(y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
        "\n",
        "Where $y$ is the actual value (0 or 1) and $\\hat{y}$ is the predicted probability. Looking closer at this equation, we can see that the first term will cancel out if $y=0$, and similarly the second term will cancel out if $y=1$. Therefore, we can write the same equation as a piecewise function.\n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=\\begin{cases}-\\log(1-\\hat{y}) & \\text{if $y=0$} \\\\-\\log(\\hat{y}) & \\text{if $y=1$}\\end{cases}$\n",
        "\n",
        "Keep in mind that $\\hat{y}$ is a decimal value in the range $(0, 1)$. The $\\log$ function returns a negative number for such values. As a result, we must take the negative of the log to return a positive value. \n",
        "\n",
        "To see why this function works as the error, try experimenting in the next code cell with different values of $y$ and $\\hat{y}$ then analyze the corresponding loss function value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YdpLQSFPUdk",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99cbb1ae-c4aa-4325-adcc-ecd375d806ef"
      },
      "source": [
        "import numpy as np\n",
        "def binary_crossentropy(y, yhat):\n",
        "  #code is derived from the piecewise function\n",
        "  if y == 0:\n",
        "    return -np.log(1.0-yhat)\n",
        "\n",
        "  if y == 1:\n",
        "    return -np.log(yhat)\n",
        "\n",
        "y = 0 #@param [0, 1] {type:\"raw\"}\n",
        "yhat = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "print(f'Loss: {binary_crossentropy(y, yhat)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.05129329438755058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uHqb3_RtMC4"
      },
      "source": [
        "##Back Propogation\n",
        "\n",
        "To simplify this process, we will show back propagation with the Sum of Squares error as our loss function. \n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=(y-\\hat{y})^{2}$\n",
        "\n",
        "Keep in mind that our goal is to find the global minimum of the loss concerning our weights. To update our weights, we first need to find out how much a small change in the weight will affect our loss function. In other words, this is what we need to find:\n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w}$\n",
        "\n",
        "However, we cannot find the derivative of $(y-\\hat{y})^2$ with respect to $w$ if it does not exist in the expression. Fortunately, we can use the chain rule to overcome this obstacle. \n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w} = \\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial{z}} * \\frac{\\partial z}{\\partial w}$\n",
        "\n",
        "As a reminder, during forward propagation, we defined $z=w \\cdot x+b$. The expanded expression can easily be simplified. \n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial{z}} * \\frac{\\partial z}{\\partial w} = -2(y-\\hat{y}) * \\sigma(z)(1-\\sigma(z)) * x$\n",
        "\n",
        "The first term, $-2(y-\\hat{y})$, and the last term, $x$, are pretty easy to derive. The middle term requires us to take the derivative of the sigmoid function. We will not derive it here, but the sigmoid derivative can be cleanly written in terms of the sigmoid function itself as:\n",
        "\n",
        "$\\sigma^\\prime(x)=\\sigma(x)(1-\\sigma(x))$\n",
        "\n",
        "The value of $\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w}$ that we solved for, gives us the value that we call a gradient. Now, we will see the graphical interpretation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XViF253ODQDz"
      },
      "source": [
        "In the graph below, the $x$-axis represents the weight, and the $y$-axis represents the function J, which is any arbitrary loss function. The value we solved for the above is called the gradient, or in simpler terms, it is the slope of the tangent line at a point. Our end goal is to reach the global cost minimum since it is the point where the loss is minimized. Here is the algorithm that we will repeat several times to achieve this task. \n",
        "\n",
        "$w = w-\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial w}$\n",
        "\n",
        "Let's think through this by using the image below. To reach the minimum, the weight needs to decrease. The slope of the tangent line/gradient is a positive value in this case. As a result, subtracting this value will help us get closer to our desired weight. \n",
        "\n",
        "<img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" alt=\"gradient_descent\" width=\"400\"/>\n",
        "\n",
        "Repeating this process several times is how a neural network trains itself. At this point, we know how to feed the inputs into a neural network and adjust the weights using back propagation! Now, it's time to transition from simple networks with just 2 layers (perceptron), to networks with additional layers in the middle. All the concepts stay the same, the only difference is that there are more weights to train.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yoLWi777Qqh"
      },
      "source": [
        "##Artificial Neural Networks\n",
        "\n",
        "Artificial Neural Networks (ANN) are very similar to Perceptrons except they have one extra layer. The figure below shows an example of the ANN. The input and output layers do not change. The layer in the middle is called the hidden layer. Before, we only had one weight matrix, connecting the input to the output. Now, we have an extra set of connections. \n",
        "\n",
        "Here is what the two-weight matrices would look like in the figure below.\n",
        "\n",
        "$W_1=\n",
        "\\begin{bmatrix} \n",
        "w_{1,1} & w_{1,2} & w_{1,3} & w_{1, 4} & w_{1, 5}\\\\\n",
        "w_{2,1} & w_{2,2} & w_{2,3} & w_{2, 4} & w_{2, 5}\\\\\n",
        "w_{3,1} & w_{3,2} & w_{3,3} & w_{3, 4} & w_{3, 5}      \n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Here $W_1$ represents the connections from the input layer to the hidden layer. Notice that the number of rows is the number of neurons in the input layer and the number of columns is the number of neurons in the hidden layer. \n",
        "\n",
        "$W_2=\n",
        "\\begin{bmatrix} \n",
        "\\beta_{1,1} & \\beta_{1,2}\\\\\n",
        "\\beta_{2,1} & \\beta_{2,2}\\\\\n",
        "\\beta_{3,1} & \\beta_{3,2}\\\\     \n",
        "\\beta_{4,1} & \\beta_{4,2}\\\\ \n",
        "\\beta_{5,1} & \\beta_{5,2}\\\\  \n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "$W_2$ is a matrix storing the weights ($\\beta$) connecting the hidden layer to the output layer. There are two columns since the output layer in the image has 2 output neurons. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SHAYL25F4M"
      },
      "source": [
        "\n",
        "![NN](https://drive.google.com/uc?export=view&id=1EHA2P4kLUQm_FkpYskyJ6QTSskjiaSeo)\n",
        "\n",
        "[2]\n",
        "\n",
        "Example of how a neural network can be visualized!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67hH3A5lxhje"
      },
      "source": [
        "# Code\n",
        "\n",
        "The following code is us building a neural network from scratch on the Wisconsin Breast Cancer dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35rh1sKy_kTV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3zOI-0YYVDb"
      },
      "source": [
        "We begin the neural network here by importing some necessary libraries that will allow us to actually create the virtual NN, and also display what goes on internally to maximize the accuracy of the NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA9v5T8kYxzz"
      },
      "source": [
        "What is the purpose of each library?\n",
        "\n",
        "  - The sklearn library is used to properly initialize the neural network and the necessary algorithm needed. \n",
        "  - From the sklearn library, we import the breast cancer dataset. \n",
        "  - We import matplotlib, pandas, and numpy which help us organize and visualize the data and outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI5bKzyq15QD"
      },
      "source": [
        "# Loading in the data\n",
        "import sklearn\n",
        "from sklearn.datasets import load_breast_cancer \n",
        "from sklearn.model_selection import train_test_split\n",
        "# Visualization\n",
        "import matplotlib as mpl   \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Building the network \n",
        "import numpy as np\n",
        "\n",
        "# Progress Bar\n",
        "import tqdm as tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #supresses warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18UQ5KSfAhDn"
      },
      "source": [
        "## Loading Dataset, Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1loUILA4N48"
      },
      "source": [
        "Adjust the slider to view different portions of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgInYb7T7XNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "18983871-626d-4aec-bdbb-6268dad004db"
      },
      "source": [
        "full_df = pd.read_csv('https://raw.githubusercontent.com/karthikb19/data/master/breastcancer.csv') #preprocessed data\n",
        "full_df.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
        "start_index = 150 #@param {type:\"slider\", min:0, max:564, step:1}\n",
        "full_df[start_index:start_index+5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>-0.320167</td>\n",
              "      <td>0.346815</td>\n",
              "      <td>-0.348429</td>\n",
              "      <td>-0.385345</td>\n",
              "      <td>1.219756</td>\n",
              "      <td>-0.539188</td>\n",
              "      <td>-0.721149</td>\n",
              "      <td>-0.579569</td>\n",
              "      <td>2.659279</td>\n",
              "      <td>-0.273259</td>\n",
              "      <td>0.054239</td>\n",
              "      <td>0.190772</td>\n",
              "      <td>0.003436</td>\n",
              "      <td>-0.122265</td>\n",
              "      <td>-0.007993</td>\n",
              "      <td>-0.785703</td>\n",
              "      <td>-0.411270</td>\n",
              "      <td>-0.043170</td>\n",
              "      <td>1.085796</td>\n",
              "      <td>-0.855568</td>\n",
              "      <td>-0.436776</td>\n",
              "      <td>-0.255213</td>\n",
              "      <td>-0.489715</td>\n",
              "      <td>-0.463884</td>\n",
              "      <td>-0.116980</td>\n",
              "      <td>-0.914547</td>\n",
              "      <td>-0.916656</td>\n",
              "      <td>-0.786396</td>\n",
              "      <td>0.477640</td>\n",
              "      <td>-1.085918</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>-1.678039</td>\n",
              "      <td>0.328198</td>\n",
              "      <td>-1.594021</td>\n",
              "      <td>-1.282659</td>\n",
              "      <td>-0.164412</td>\n",
              "      <td>0.495752</td>\n",
              "      <td>0.543639</td>\n",
              "      <td>-0.702606</td>\n",
              "      <td>1.498279</td>\n",
              "      <td>2.808612</td>\n",
              "      <td>-0.763969</td>\n",
              "      <td>1.351952</td>\n",
              "      <td>-0.803464</td>\n",
              "      <td>-0.662847</td>\n",
              "      <td>1.796413</td>\n",
              "      <td>1.603016</td>\n",
              "      <td>1.513163</td>\n",
              "      <td>-0.255665</td>\n",
              "      <td>0.308472</td>\n",
              "      <td>3.020373</td>\n",
              "      <td>-1.486271</td>\n",
              "      <td>0.658341</td>\n",
              "      <td>-1.464904</td>\n",
              "      <td>-1.108862</td>\n",
              "      <td>1.342755</td>\n",
              "      <td>1.124281</td>\n",
              "      <td>1.275717</td>\n",
              "      <td>-0.545359</td>\n",
              "      <td>0.681481</td>\n",
              "      <td>3.582864</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>-1.248609</td>\n",
              "      <td>-0.919110</td>\n",
              "      <td>-1.161112</td>\n",
              "      <td>-1.008772</td>\n",
              "      <td>0.771413</td>\n",
              "      <td>1.052926</td>\n",
              "      <td>4.042709</td>\n",
              "      <td>0.764814</td>\n",
              "      <td>2.688487</td>\n",
              "      <td>4.275833</td>\n",
              "      <td>1.513443</td>\n",
              "      <td>2.625622</td>\n",
              "      <td>0.597473</td>\n",
              "      <td>0.209301</td>\n",
              "      <td>1.309727</td>\n",
              "      <td>3.933610</td>\n",
              "      <td>12.072680</td>\n",
              "      <td>6.649601</td>\n",
              "      <td>1.806213</td>\n",
              "      <td>9.851593</td>\n",
              "      <td>-1.087016</td>\n",
              "      <td>-1.007551</td>\n",
              "      <td>-1.078879</td>\n",
              "      <td>-0.879102</td>\n",
              "      <td>-0.138898</td>\n",
              "      <td>0.145898</td>\n",
              "      <td>2.635815</td>\n",
              "      <td>0.647036</td>\n",
              "      <td>0.335276</td>\n",
              "      <td>2.324925</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>-0.845593</td>\n",
              "      <td>-1.445027</td>\n",
              "      <td>-0.869073</td>\n",
              "      <td>-0.776409</td>\n",
              "      <td>0.083955</td>\n",
              "      <td>-1.008427</td>\n",
              "      <td>-0.866033</td>\n",
              "      <td>-0.801139</td>\n",
              "      <td>0.067109</td>\n",
              "      <td>-0.247742</td>\n",
              "      <td>-0.649918</td>\n",
              "      <td>-0.789881</td>\n",
              "      <td>-0.711388</td>\n",
              "      <td>-0.546898</td>\n",
              "      <td>0.659367</td>\n",
              "      <td>-0.921794</td>\n",
              "      <td>-0.660943</td>\n",
              "      <td>-0.578137</td>\n",
              "      <td>0.404124</td>\n",
              "      <td>-0.823039</td>\n",
              "      <td>-0.886145</td>\n",
              "      <td>-1.527023</td>\n",
              "      <td>-0.923695</td>\n",
              "      <td>-0.773100</td>\n",
              "      <td>0.075898</td>\n",
              "      <td>-1.046800</td>\n",
              "      <td>-0.964439</td>\n",
              "      <td>-0.906686</td>\n",
              "      <td>-0.067552</td>\n",
              "      <td>-0.899167</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>-0.277565</td>\n",
              "      <td>-0.919110</td>\n",
              "      <td>-0.274287</td>\n",
              "      <td>-0.329885</td>\n",
              "      <td>-0.179357</td>\n",
              "      <td>-0.366919</td>\n",
              "      <td>0.051861</td>\n",
              "      <td>-0.363415</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>-0.103146</td>\n",
              "      <td>-0.484255</td>\n",
              "      <td>-0.769560</td>\n",
              "      <td>-0.518326</td>\n",
              "      <td>-0.386066</td>\n",
              "      <td>0.514361</td>\n",
              "      <td>-0.296669</td>\n",
              "      <td>-0.047206</td>\n",
              "      <td>-0.366616</td>\n",
              "      <td>0.865433</td>\n",
              "      <td>-0.119491</td>\n",
              "      <td>-0.310456</td>\n",
              "      <td>-0.843079</td>\n",
              "      <td>-0.285682</td>\n",
              "      <td>-0.357354</td>\n",
              "      <td>0.676449</td>\n",
              "      <td>-0.182350</td>\n",
              "      <td>0.137744</td>\n",
              "      <td>-0.264733</td>\n",
              "      <td>1.534051</td>\n",
              "      <td>0.132121</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  ...  worst fractal dimension  target\n",
              "150    -0.320167      0.346815  ...                -1.085918       1\n",
              "151    -1.678039      0.328198  ...                 3.582864       1\n",
              "152    -1.248609     -0.919110  ...                 2.324925       1\n",
              "153    -0.845593     -1.445027  ...                -0.899167       1\n",
              "154    -0.277565     -0.919110  ...                 0.132121       1\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaH_eHlDwlpE"
      },
      "source": [
        "- Here we are loading our breast cancer data inside of a dataframe to better visualize the features and labels of our data. This is also important as it gives us easy access to our data in the form of an array so that we can extract whatever data is necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-gNuPbovUnu"
      },
      "source": [
        "X_train = full_df.drop('target', inplace=False, axis=1) #remove 'target' column from input features\n",
        "y_train = full_df['target'] #stores target (1 or 0) in a separate array\n",
        "\n",
        "#since we shuffled, the index numbers were messed up, this resets them\n",
        "X_train = X_train.reset_index(drop=True) \n",
        "y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "#convert to numpy arrays with float values\n",
        "X_train = np.array(X_train, dtype=float)\n",
        "y_train = np.array(y_train, dtype=float)\n",
        "\n",
        "#reshape y_train to make matrix multiplication possible\n",
        "y_train = np.array(y_train).reshape(-1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNgdAbz6F7Z9"
      },
      "source": [
        "Here the data (within numpy arrays) is being trained so that the model has experience classifying whether the breast cancer is benign or malignant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVtKxNvFx9U2"
      },
      "source": [
        "# Initalizing Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiwNHC5jOoff"
      },
      "source": [
        "class Perceptron:\n",
        "  def __init__(self, x, y):\n",
        "    \n",
        "    self.input = np.array(x, dtype=float) \n",
        "    self.label = np.array(y, dtype=float)\n",
        "    self.weights = np.random.rand(x.shape[1], y.shape[1]) #randomly initialize the weights\n",
        "    self.z = self.input@self.weights #dot product of the vectors\n",
        "    self.yhat = self.sigmoid(self.z) #apply activation function\n",
        "\n",
        "    \n",
        "  def sigmoid(self, x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "  def sigmoid_deriv(self, x):\n",
        "    s = sigmoid(x)\n",
        "    return s(1-s)\n",
        "\n",
        "  def forward_prop(self):\n",
        "    self.yhat = self.sigmoid(self.input @ self.weights) #@ symbol represents matrix multiplication (also works for vectors)\n",
        "    return self.yhat\n",
        "\n",
        "  def back_prop(self):\n",
        "    gradient = self.input.T @ (-2.0*(self.label - self.yhat)*self.sigmoid(self.yhat))  #self.input is the x value\n",
        "    \n",
        "    self.weights -= gradient #process of finding the minimum loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQP8SXwQGLY1"
      },
      "source": [
        "Initializing weights is an important step of a neural network as the neural network needs values to adjust so that it can create a more balanced and efficient neural network. Weights act as the inputs for the activation functions and are essentially the value that each neuron provides into the neural network which is constantly tweaked with each passing epoch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04a3fDz3Gdyj"
      },
      "source": [
        "Here we also define our activation functions which serve the purpose of interpreting the data to feed it into the next layer of the neural network. Activation functions are necessary because most data is not linear, so there need to be specialized functions that can deal with more complicated \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9CrehG0AmoT"
      },
      "source": [
        "## Fitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHH94y2EPcBz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0c5c0e55-04e3-415d-f49d-5af86739e538"
      },
      "source": [
        "simple_nn = Perceptron(X_train, y_train)\n",
        "training_iterations = 1000\n",
        "\n",
        "\n",
        "for i in range(training_iterations):\n",
        "  simple_nn.forward_prop()\n",
        "  simple_nn.back_prop()\n",
        "\n",
        "yhat = simple_nn.forward_prop()\n",
        "\n",
        "def mse(yhat, y):\n",
        "  sum = 0.0\n",
        "  for pred, label in zip(yhat, y):\n",
        "    sum += (pred-label)**2\n",
        "  return sum/len(yhat)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse(yhat, simple_nn.label)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 1)\n",
            "Mean Squared Error: [0.01572726]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQPaHnsISGbB"
      },
      "source": [
        "We highly recommend that you try experimenting with ```training_iterations``` parameter to see how the number of iterations affects the mean squared error. Hopefully, you should notice that the error goes down as you increase the number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6pBL6s7a_jY"
      },
      "source": [
        "# Feature Reduction Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puIf4uY3bIKr"
      },
      "source": [
        "In complex datasets, having a lot of different variables to deal with to raise the accuracy and usefulness of the model does not result in all the features being used. For example, let us look at the logistic regression equation for features:\n",
        "\n",
        "$\n",
        "Y = \\sigma(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...)\n",
        "$\n",
        "\n",
        "- Y is the output of the logistic function\n",
        "- $\\sigma$ is the sigmoid activation function\n",
        "  - Sigmoid is used here since we want to create a logistic regression, where the features are compressed to values between 0 and 1.\n",
        "- $\\beta$ are our coefficients for each of the features $x$\n",
        "\n",
        "Following the training sequence of the model, if any of the $\\beta$'s equal 0, we know that their corresponding $x$ feature was not used to make adjustments to the model. To make this clear realize that ***The amount of different features is used is equivalent to the number of non-zero coefficients ($\\beta$ values).***\n",
        "\n",
        "The purpose of us performing this feature reduction to get rid of unnecessary $\\beta$ and $x$ values is to prevent something called overfitting from happening. Overfitting is when our loss function gets stuck in a local minimum instead of trying to locate the maximum-minimum possible for a specific scenario. Now let's go over the different ways we can go about solving this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdxqHdACWTca"
      },
      "source": [
        "## Lasso L1 Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk5DxVsbWhZI"
      },
      "source": [
        "A primary method used to reduce the amount of features for a machine learning mdodel is to implement a Lasso L1 penalty to the function. Using the logistic regresison equation that we identified earlier:\n",
        "\n",
        "$\n",
        "Y = \\sigma(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...)\n",
        "$\n",
        "\n",
        "We can implement a Lasso penalty that attempts to reduce as many $\\beta$ values as possible to 0. As a result of this, only the resulting $x$ features that remain due to non-zero $\\beta$ coefficients will be implemented into the model, removing unnecessary features which in turn helps solve our overfitting problem. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1OiScfT_C41xzSsTuS3CM-Xp8dKJDL4oJ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A5N4YqzXrCX"
      },
      "source": [
        "***Mathematical Representation***\n",
        "\n",
        "This penalty can be mathematically represented using the concepts of loss functions:\n",
        "\n",
        "$$\n",
        "Loss = Cost(X, Y, \\beta) + \\lambda \\sum_{i}|\\beta_i|\n",
        "$$\n",
        "Where the cost function is represented by:\n",
        "$$Cost = \\sum_{i=1}^{n}(Y_i-\\sum_{j=1}^{p}X_{ij}\\beta_j)^2$$\n",
        "\n",
        "Including different values of $\\lambda$ will let more or less of the coefficients ($\\beta$ values) to 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfSWLTpziLF2"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTAv7aSViSkn"
      },
      "source": [
        "Cross validation is a method used to determine what the best value of $\\lambda$ is. Cross validation involves splitting the data into training and testing sets using different values of $\\lambda$ . The model is then trained using different values of this $\\lambda$ and the accuracy is then evaluated using the training test. This is done for a certain $k$ iterations for the splitting of the training and tested data. Then the $\\lambda$ value that had the best accuracy for the training output is implemented into the model.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1280px-K-fold_cross_validation_EN.svg.png\" alt=\"drawing\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3hfyzLhnmbR"
      },
      "source": [
        "## Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y71Xu_XhnqUN"
      },
      "source": [
        "Principal Component Analysis (PCA) is an unsupervised learning technique used to reduce the number of features in a dataset in yet another way. In applications such as biology and genomics, there is an abundance of features, so this technique uses linear combinations of the original features to make new features. This is very useful to reduce the number of dimensions of a machine learning model so that overfitting is prevented, and the model is easier to interpret and comprehend for both the computer and the programmer. \n",
        "\n",
        "<img src=\"http://phdthesis-bioinformatics-maxplanckinstitute-molecularplantphys.matthias-scholz.de/fig_pca_illu3d.png\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "Here is a sample table that can help us visualize how features can be linearly combined. The table below represents some statistics about high school swimmers. This data set will have the features ```Height```, ```Weight```, ```Freestyle```, and ```Backstroke```.\n",
        "\n",
        "|Name |Height |Weight |Freestyle | Backstroke|\n",
        "|--|--|--|--|--|\n",
        "|Harry|65 in|110 lb|30 sec|26 sec|\n",
        "|Joyce|70 in |95 lb|28 sec|27 sec|\n",
        "|Troy| 54 in|100 lb|32 sec|33 sec|\n",
        "|Mary|57 in|105 lb|40 sec|42 sec|\n",
        "\n",
        "PCA allows us to develop linear combinations of the features that spread the data furthest apart on the lowest dimension grid possible in order to uncomplicate the data. You may be wondering, why do we need to do all this in the first place:\n",
        "- features correspond to dimensions, and if we have 1000 features, this means that our data can only be represented on a 1000 dimension graph which is impossibly difficult for humans to understand and tweak so that the model is more efficient\n",
        "- excessive features also cause overfitting which cases the loss function to be trapped at a local minima instead of locating the global minimum of the loss\n",
        "\n",
        "If we use this PCA to reduce the space from 4 to 2 features as in our example, PCA will show us what the new best features to use are:\n",
        "\n",
        "|Name |Height + Weight | Freestyle + Backstroke|\n",
        "|--|--|--|\n",
        "|Harry|175 in-lb|56 sec|\n",
        "|Joyce|165 in-lb|55 sec|\n",
        "|Troy| 154 in-lb|65 sec|\n",
        "|Mary|162 in-lb|82 sec|\n",
        "\n",
        "\n",
        "As we can see in this example, as a result of Principal Component Analysis, the 4 features were combined together to form 2 groups of 2 features, which shows us how our graph was instantly reduced from 4 dimensions all the way down 2 dimensions which are very easy to visualize since it only involves an $x$ and $y$ axis, something that is very fundamental that most people understand. \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csOS6bJixisJ"
      },
      "source": [
        "# Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7Zxg7R154s"
      },
      "source": [
        "In this section, we will cover some applications of neural networks. These will be \n",
        "\n",
        "\n",
        "*   Medicine\n",
        "*   Robotics\n",
        "*   Finance\n",
        "*   Understanding Natural Language\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz5oZRonr9DQ"
      },
      "source": [
        "## Medicine "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt8Rz3eTtSBC"
      },
      "source": [
        "There are many applications of ML/DL to medicine and I will name and describe a few here. \n",
        "\n",
        "### Disease Identification\n",
        "\n",
        "Disease identification is an important sub-field of AI and Medicine. It works by collecting a set of images(generally by scraping the web, or datasets such as this [one](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) on Kaggle) and training a more specific neural network known as a convolutional neural network(CNN). CNN's are extremely accurate and are made for image recognition and identification tasks. \n",
        "\n",
        "The founder of StartOnAI, Sid Sharma, recently wrote a paper called [\"DermaDetect A computer vision model for an accurate diagnosis of skin conditions and rashes\"](https://www.dropbox.com/s/hc5yqap7spo44ip/DermaDetect.pdf?dl=0). The goal of DermaDetect was to detect skin rashes/diseases using computer vision, and it describes in detail what Sid went through to get a highly accurate model. Sid used many states of the art techniques, and eventually even beat out the state of the art model in skin detection(determined by using Google AutoML). \n",
        "\n",
        "### Clinical Trial Research\n",
        "Identifying suitable for clinical trials is often difficult, but if we start using machine learning for predictive analysis on which candidates to select, we could access more data than we ever have. For instance, instead of just using genetic information, and family history, we could start using doctors' visits and even social media!\n",
        "\n",
        "### Health Records\n",
        "Over the past few months, we have seen the effect of COVID-19 first hand. Every day, thousands of researchers around the world and trying to find vaccines to cure and stop the spread of this deadly virus. And with the many initiatives going around, there has been a plethora of research papers being written, and the goal of these researchers is to get tangible information from the research papers which is where Natural Language Processing(NLP) comes into play. NLP allows computers to understand the text they see, and with the speed of computers, combining them with NLP will allow us to get new information extremely efficiently. \n",
        "\n",
        "If you want to be part of this initiative click this [link](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtLNsxvhr_Dn"
      },
      "source": [
        "## Robotics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0E_K8cEKbZV"
      },
      "source": [
        "Next, let us talk about some important instances of robotics in the field of ML/DL.\n",
        "\n",
        "### Computer Vision\n",
        "Robotics and computer vision go hand in hand. For instance, many of the companies in the self-driving cars field, such as CommaAI, Tesla, and Uber, use various computer vision techniques to map out the environment in which the car is traveling in. With the amount of data, we take in when driving, we can simply use that data and feed it to a  vision model to help various types of robots to also understand and visualize the world! Along with this, we can use robots to find anomaly’s in structures such as buildings using computer vision. \n",
        "\n",
        "\n",
        "### Reinforcement Learning\n",
        "Recently, reinforcement learning has been all the rage with many companies especially Google’s Deep Mind creating extremely successful RL bots in games such as Go, Chess, and Shogi. But now the time has come to apply the same skills used in those mentally challenging games into real life, by creating robots that can learn and understand the world through trial and error, and the way they understand the world is, you guessed it computer vision! With robotics, we can help use robots effectively in the real world! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSuwyZPusB-n"
      },
      "source": [
        "## Finance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjJE64JyRO05"
      },
      "source": [
        "### Stock Market Trading\n",
        "Many people purchase stocks, and often they do it based on recent trends, and sentiment about a specific company, and they use this information to predict when to buy/sell stocks. This is when you could use recurrent neural networks(RNN’s) for sentiment analysis. We could look through the internet and find articles of a particular company by searching for the company’s name! Then we can take subsections of the beginning, middle, and end and assess their sentiment and give investors an accurate notification of when to buy or sell a particular stock."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNgXJzBjxhqc"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbLzTbkIATBy"
      },
      "source": [
        "[1] https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
        "\n",
        "[2] https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c\n",
        "\n",
        "[3] https://emerj.com/ai-sector-overviews/machine-learning-in-robotics/\n",
        "\n",
        "[4] https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
        "\n",
        "[5] https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "\n",
        "[6] https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
        "\n",
        "[7] https://www.nature.com/articles/s41563-019-0360-1\n",
        "\n",
        "[8] https://medium.com/@ocktavia/titanic-prediction-with-artificial-neural-network-in-r-5dd20fb98dea\n",
        "\n",
        "[9] https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
        "\n",
        "[10] https://towardsdatascience.com/what-is-a-perceptron-210a50190c3b"
      ]
    }
  ]
}